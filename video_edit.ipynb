{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from misc_utils.train_utils import unit_test_create_model\n",
    "from misc_utils.image_utils import save_tensor_to_gif, save_tensor_to_images\n",
    "config_path = 'configs/instruct_v2v_inference.yaml'\n",
    "diffusion_model = unit_test_create_model(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt = torch.load('insv2v.pth', map_location='cpu')\n",
    "diffusion_model.load_state_dict(ckpt, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit params\n",
    "EDIT_PROMPT = 'make the car red Porsche and drive alone beach'\n",
    "VIDEO_CFG = 1.2\n",
    "TEXT_CFG = 7.5\n",
    "LONG_VID_SAMPLING_CORRECTION_STEP = 0.5\n",
    "\n",
    "# video params\n",
    "VIDEO_PATH = 'data/car-turn.mp4'\n",
    "IMGSIZE = 384\n",
    "NUM_FRAMES = 32\n",
    "VIDEO_SAMPLE_RATE = 10\n",
    "\n",
    "# sampling params\n",
    "FRAMES_IN_BATCH = 16\n",
    "NUM_REF_FRAMES = 4\n",
    "USE_MOTION_COMPENSATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pl_trainer.inference.inference import InferenceIP2PVideo, InferenceIP2PVideoOpticalFlow\n",
    "if USE_MOTION_COMPENSATION:\n",
    "    inf_pipe = InferenceIP2PVideoOpticalFlow(\n",
    "        unet = diffusion_model.unet,\n",
    "        num_ddim_steps=20,\n",
    "        scheduler='ddpm'\n",
    "    )\n",
    "else:\n",
    "    inf_pipe = InferenceIP2PVideo(\n",
    "        unet = diffusion_model.unet,\n",
    "        num_ddim_steps=20,\n",
    "        scheduler='ddpm'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.single_video_dataset import SingleVideoDataset\n",
    "dataset = SingleVideoDataset(\n",
    "    video_file=VIDEO_PATH,\n",
    "    video_description='',\n",
    "    sampling_fps=VIDEO_SAMPLE_RATE,\n",
    "    num_frames=NUM_FRAMES,\n",
    "    output_size=(IMGSIZE, IMGSIZE)\n",
    ")\n",
    "batch = dataset[20] # start from 20th frame\n",
    "batch = {k: v.cuda()[None] if isinstance(v, torch.Tensor) else v for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_batch(cond, frames_in_batch=16, num_ref_frames=4):\n",
    "    frames_in_following_batch = frames_in_batch - num_ref_frames\n",
    "    conds = [cond[:, :frames_in_batch]]\n",
    "    frame_ptr = frames_in_batch\n",
    "    num_ref_frames_each_batch = []\n",
    "\n",
    "    while frame_ptr < cond.shape[1]:\n",
    "        remaining_frames = cond.shape[1] - frame_ptr\n",
    "        if remaining_frames < frames_in_batch:\n",
    "            frames_in_following_batch = remaining_frames\n",
    "        else:\n",
    "            frames_in_following_batch = frames_in_batch - num_ref_frames\n",
    "        this_ref_frames = frames_in_batch - frames_in_following_batch\n",
    "        conds.append(cond[:, frame_ptr:frame_ptr+frames_in_following_batch])\n",
    "        frame_ptr += frames_in_following_batch\n",
    "        num_ref_frames_each_batch.append(this_ref_frames)\n",
    "\n",
    "    return conds, num_ref_frames_each_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = [diffusion_model.encode_image_to_latent(frames) / 0.18215 for frames in batch['frames'].chunk(16, dim=1)] # when encoding, chunk the frames to avoid oom in vae, you can reduce the 16 if you have a smaller gpu\n",
    "cond = torch.cat(cond, dim=1)\n",
    "text_cond = diffusion_model.encode_text([EDIT_PROMPT])\n",
    "text_uncond = diffusion_model.encode_text([''])\n",
    "conds, num_ref_frames_each_batch = split_batch(cond, frames_in_batch=FRAMES_IN_BATCH, num_ref_frames=NUM_REF_FRAMES)\n",
    "splitted_frames, _ = split_batch(batch['frames'], frames_in_batch=FRAMES_IN_BATCH, num_ref_frames=NUM_REF_FRAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First video clip\n",
    "cond1 = conds[0]\n",
    "latent_pred_list = []\n",
    "init_latent = torch.randn_like(cond1)\n",
    "latent_pred = inf_pipe(\n",
    "    latent = init_latent,\n",
    "    text_cond = text_cond,\n",
    "    text_uncond = text_uncond,\n",
    "    img_cond = cond1,\n",
    "    text_cfg = TEXT_CFG,\n",
    "    img_cfg = VIDEO_CFG,\n",
    ")['latent']\n",
    "latent_pred_list.append(latent_pred)\n",
    "\n",
    "\n",
    "# Subsequent video clips\n",
    "for prev_cond, cond_, prev_frame, curr_frame, num_ref_frames_ in zip(\n",
    "    conds[:-1], conds[1:], splitted_frames[:-1], splitted_frames[1:], num_ref_frames_each_batch\n",
    "):\n",
    "    init_latent = torch.cat([init_latent[:, -num_ref_frames_:], torch.randn_like(cond_)], dim=1)\n",
    "    cond_ = torch.cat([prev_cond[:, -num_ref_frames_:], cond_], dim=1)\n",
    "    if USE_MOTION_COMPENSATION:\n",
    "        ref_images = prev_frame[:, -num_ref_frames_:]\n",
    "        query_images = curr_frame\n",
    "        additional_kwargs = {\n",
    "            'ref_images': ref_images,\n",
    "            'query_images': query_images,\n",
    "        }\n",
    "    else:\n",
    "        additional_kwargs = {}\n",
    "    latent_pred = inf_pipe.second_clip_forward(\n",
    "        latent = init_latent, \n",
    "        text_cond = text_cond,\n",
    "        text_uncond = text_uncond,\n",
    "        img_cond = cond_,\n",
    "        latent_ref = latent_pred[:, -num_ref_frames_:],\n",
    "        noise_correct_step = LONG_VID_SAMPLING_CORRECTION_STEP,\n",
    "        text_cfg = TEXT_CFG,\n",
    "        img_cfg = VIDEO_CFG,\n",
    "        **additional_kwargs,\n",
    "    )['latent']\n",
    "    latent_pred_list.append(latent_pred[:, num_ref_frames_:])\n",
    "\n",
    "# Save GIF\n",
    "latent_pred = torch.cat(latent_pred_list, dim=1)\n",
    "image_pred = diffusion_model.decode_latent_to_image(latent_pred).clip(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images = batch['frames'].cpu()\n",
    "transferred_images = image_pred.float().cpu()\n",
    "concat_images = torch.cat([original_images, transferred_images], dim=4)\n",
    "\n",
    "save_tensor_to_gif(concat_images, 'results/video_edit.gif', fps=5)\n",
    "save_tensor_to_images(transferred_images, 'results/video_edit_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the gif\n",
    "from IPython.display import Image\n",
    "Image(filename='results/video_edit.gif')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
